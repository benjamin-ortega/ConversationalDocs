# Desaf√≠o T√©cnico: Copiloto Conversacional sobre Documentos

## Descripci√≥n del Proyecto

Este proyecto es un copiloto conversacional dise√±ado para interactuar con hasta 5 archivos PDF subidos por el usuario. La aplicaci√≥n permite hacer preguntas en lenguaje natural sobre el contenido de los documentos, as√≠ como realizar tareas de an√°lisis avanzado como res√∫menes, comparaciones y clasificaci√≥n autom√°tica por temas.

## Instrucciones para Levantar el Entorno

Para ejecutar la aplicaci√≥n, tener **Docker** y **Docker Compose** instalados.

1.  **Configurar la API Key:**
    Crea un archivo .env en la ra√≠z del proyecto y a√±ade tu clave de la API de Google Gemini

    Para obtener tu clave: Ve a Google AI Studio y sigue las instrucciones para crear una nueva clave de API.

    ```env
    GEMINI_API_KEY=tu_clave_de_api_aqui
    ```

2.  **Levantar la aplicaci√≥n con Docker Compose:**
    Ejecuta el siguiente comando para construir la imagen y arrancar el contenedor.

    ```bash
    docker compose up --build
    ```

3.  **Acceder a la aplicaci√≥n:**
    Abre tu navegador y navega a la siguiente URL:
    [http://localhost:8501]

---

## Arquitectura del Sistema

El sistema sigue una arquitectura modular y estructurada, dividida en un **backend** (l√≥gica de procesamiento) y un **frontend** (interfaz de usuario).

1.  **Frontend (app.py):**
    Construido con **Streamlit**, provee la interfaz para la interacci√≥n del usuario. Gestiona la subida de archivos, el flujo conversacional y las opciones de an√°lisis avanzado a trav√©s de botones y selectores. El estado de la aplicaci√≥n se mantiene en `st.session_state`.

2.  **Backend (core/backend.py):**
    Contiene toda la l√≥gica de negocio y las integraciones, funcionando como el motor del sistema. Se encarga de:
    - **Procesamiento de Documentos:** Utiliza la librer√≠a `PyMuPDFLoader` para cargar los PDFs. El texto extra√≠do se divide en fragmentos manejables con `RecursiveCharacterTextSplitter`.
    - **Almacenamiento Vectorial:** Emplea **ChromaDB** para almacenar los **embeddings** (representaciones num√©ricas) de los fragmentos de texto, lo que permite una b√∫squeda sem√°ntica eficiente. Los embeddings se generan con `GoogleGenerativeAIEmbeddings`.
    - **Orquestaci√≥n del LLM:** Usa **LangChain** como un framework de orquestaci√≥n. Para el chat conversacional, se implementa una `ConversationalRetrievalChain` que gestiona el flujo de la pregunta, la recuperaci√≥n de contexto y la generaci√≥n de la respuesta.
    - **Modelo de Lenguaje (LLM):** La API de **Gemini 1.5 Pro** se usa para la generaci√≥n de respuestas. Una clase personalizada, `GeminiAPIWrapper`, encapsula la l√≥gica de la API para mantener el c√≥digo limpio y modular.

---

## Justificaci√≥n de Elecciones T√©cnicas

- **Python + Streamlit:** Proporciona un stack de desarrollo r√°pido para prototipos de IA, permitiendo construir una interfaz funcional en poco tiempo.
- **LangChain:** Es un framework de orquestaci√≥n maduro y flexible. Sus "chains" y "retrievers" permiten una estructura clara y un flujo de trabajo extensible, facilitando la adici√≥n de nuevas funcionalidades.
- **ChromaDB:** Un vector store ligero y de c√≥digo abierto que se integra f√°cilmente con LangChain. Su naturaleza "en memoria" (por defecto) es ideal para este tipo de desaf√≠o donde los datos no necesitan persistir entre sesiones.
- **Google Gemini 1.5 Pro:** Elegido por su rendimiento avanzado en razonamiento complejo y la capacidad de manejar grandes contextos, lo que es crucial para analizar documentos extensos. La implementaci√≥n de una clase `GeminiAPIWrapper` encapsula la l√≥gica de la API, manteniendo el c√≥digo limpio y desacoplado.
- **Docker + Docker Compose:** Garantiza que el entorno de la aplicaci√≥n sea reproducible en cualquier m√°quina. Simplifica la configuraci√≥n de dependencias, la API key y el arranque del servicio.

---

## üó£Ô∏è Flujo Conversacional

El flujo sigue un ciclo simple de "Pregunta -> B√∫squeda -> Respuesta".

1.  **Vectorizaci√≥n de Documentos:** Al procesar los PDFs, el contenido se divide en fragmentos y se convierte en representaciones num√©ricas (`embeddings`) que se almacenan en el `vectorstore` (ChromaDB).
2.  **Pregunta del Usuario:** El usuario introduce una pregunta en la interfaz de chat.
3.  **B√∫squeda de Contexto:** La cadena conversacional utiliza el `vectorstore` para encontrar los fragmentos de texto m√°s relevantes para la pregunta del usuario. Se consideran tanto la pregunta actual como el historial de la conversaci√≥n.
4.  **Generaci√≥n de Respuesta:** La pregunta y los fragmentos de texto relevantes se env√≠an a la API de Google Gemini como un solo prompt. El LLM utiliza este contexto para generar una respuesta coherente y precisa.
5.  **Historial y Memoria:** La `ConversationBufferMemory` de LangChain almacena el historial del chat para que las respuestas futuras sean m√°s contextuales y mantengan el hilo de la conversaci√≥n.

---

## Limitaciones y Mejoras Futuras

- **Escalabilidad:** El `vectorstore` actual (ChromaDB en memoria) no es escalable para un gran volumen de documentos o usuarios. Una mejora ser√≠a migrar a una soluci√≥n de nube como Weaviate o Qdrant.
- **Manejo de Errores:** Aunque se manejan errores de la API, se podr√≠an a√±adir m√°s validaciones y mensajes de error m√°s espec√≠ficos para una mejor experiencia de usuario.
- **Tipos de Archivos:** Actualmente solo se aceptan PDFs. Se podr√≠a extender la funcionalidad para incluir otros formatos como `.docx` o `.txt`.
